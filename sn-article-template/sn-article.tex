%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove �Numbered� in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Article Title}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1,2]{\fnm{First} \sur{Author}}\email{iauthor@gmail.com}

\author[2,3]{\fnm{Second} \sur{Author}}\email{iiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{100190}, \state{State}, \country{Country}}}

\affil[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

\affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. Authors are advised to check the author instructions for the journal they are submitting to for word limits and if structural elements like subheadings, citations, or equations are permitted.}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{keyword1, Keyword2, Keyword3, Keyword4}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

Planning for complex surgical procedures presents numerous challenges and difficulties, primarily due to the intricate and variable nature of human anatomy. The success of such interventions heavily relies on the surgeon's ability to anticipate and navigate these complexities, underscoring the importance of integration of advanced imaging and planning tools that can provide a detailed and accurate representation of the patient-specific anatomy \cite{RN30}.

Virtual reality (VR) offers a promising solution to the challenges inherent in complex surgery planning \cite{RN32, RN31, RN2, RN24}. By providing immersive, three-dimensional (3D) visualizations of patient-specific anatomy, VR enables surgeons to interact with and manipulate anatomical models in a immersive nature. This interactive capability allows for a more thorough visuospatial conversion from two-dimensional (2D) images, optimizing strategic planning. Supported by advancements in computing power and hardware \cite{RN33}, applications of VR in preoperative planning have demonstrated clinical benefits for both patients and physicians, including changes in preoperative planning \cite{RN17, RN13, RN37, RN10, RN44, RN29}, improvements in surgical decision-making \cite{RN5}, and reductions in operative times \cite{RN36, RN28} across multiple surgical subspecialties \cite{RN38, RN2}.

Integration with communication technologies can further enhance the convenience and accessibility of preoperative planning. The flexibility provided by enabling real-time interaction between multidisciplinary teams ensures that critical insights and decisions can be made without the need for all team members to be physically present in the same location \cite{RN26, RN6}. The combination of virtual reality with network communication can facilitate mutual understanding and collaboration among surgical team members, which is crucial for both patient outcomes and physician efficiency \cite{RN62, RN25}.

In this work, we develop a VR surgical planning system, ColabSurgVR. This system allows multiple users to examine the patient organ model reconstructed from computed tomography (CT) dataset and collaborate on forming a preoperative plan in real time. Also we describe a protocol to process patient-specific image datas into 3D models for immersive interaction in virtual environment. To test the use case and performance, we have conducted a pilot user study involving 10 physicians to examine real world patient data present the preliminary qualitative result.

\section{System Design and Implementation}
Our proposed pipeline takes the imaging data from CT and generates a 3D model for viewing in virtual reality. It involves processing steps to segment the content of the images and reconstrct the anatomy for the patient. The resultant 3D models will be further modified and then imported into our VR surgical planning system supporting immersive visulization and intuitive interaction. The user can assess the system either by extended reality or conventional devices. See Figure \ref{fig:Scheme} for schematic representation of our workflow and system architecture.



\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{../Media/Scheme}  
  \caption{System architecture of ColabSurgVR.}
  \label{fig:Scheme}
\end{figure}

\subsection{Medical Image Acquisition}
Volumetric data acquired from computer tomography (CT) scanners was output in the Digital Imaging and Communication (DICOM) format. The slice thickness was set to 1 mm for acquisition protocol, and all images were reconstructed into 3 mm slices for subsequent interpretation and analysis.
\subsection{Segmentation and Virtual Reality Object Generation}
The DICOM images were anonymized and then imported into a commercially available medical imaging workstation, Synapse 3D (Fujifilm, Tokyo, Japan), for 3D visualization, segmentation, and 3D model generation. Skin, bones, vascular structures, bronchi, bronchopulmonary segments, and tumors (if present) were segmented from the CT datasets semiautomatically using built-in extraction functions in Synapse 3D Viewer and Lung Analysis Resection applications. Additional segmentation of small branches and border modification were performed manually by assigning or deleting pixels in the image dataset to the corresponding desired anatomic structures. Isolation of submodels (e.g., isolating a rib from the bone models) was also conducted by manually dividing the segmented data. During manual refinement, the CT image data with adjustable window settings and a 3D volumetric rendering of the segmented region were both available to the operator for optimal evaluation and stereoscopic visualization. After segmentation, texture mapping was applied to define surface texture and color information of the segmented data. The resulting data were then exported into a standard tessellation language (STL) file format.
\subsection{Model Optimization}
To optimize the mesh representation of the 3D models, an open-source 3D mesh processing software, MeshLab (version 2023.12), was utilized \cite{RN19}. The STL files were imported into MeshLab and first underwent a series of cleaning operations, including the removal of duplicated vertices, unreferenced vertices, and zero-area faces to enhance the mesh integrity. Then, quadric edge collapse decimation targeting a 50$\%$ reduction in face count was applied to reduce the polygon count while preserving essential geometric features \cite{RN20}. Laplacian smoothing was applied to ensure balanced surface smoothness. Normals were recomputed to correct any lighting and shading inconsistencies using weighted normal calculation. Finally, isolated mesh components were removed, with the minimum component size set to 10$\%$ of the overall model diameter. The optimized meshes were then exported in OBJ format for integration into our VR surgery planning system.
\subsection{VR Environment Development}
For the presentation and interaction with the 3D models, we developed our software using the Unity 3D engine (Unity Technologies, San Francisco, CA, version 2020.3) and integrated it with the Meta XR All-in-One SDK (version 60). We employed the Universal Rendering Pipeline (URP) from Unity, which facilitated optimized graphics performance across various platforms, including mobile devices, PCs, and head-mounted displays (HMDs) utilized in our study.
The software was deployed on an Omen 16 laptop (HP Inc., Palo Alto, California) featuring an Intel® Core™ i7-12700H CPU at 2.30 GHz, 16 GB of RAM, and an NVIDIA® GeForce™ RTX 3070 graphics card. For an immersive virtual reality experience, we used the Meta Quest Pro and Meta Quest 3 HMDs (Meta, Menlo Park, California), along with their corresponding controllers. The Meta Quest Pro offers a resolution of 1800 x 1920 pixels per eye, a refresh rate of 72/90 Hz, and a field of view of 106 degrees. The Meta Quest 3 enhances these specifications with a resolution of 2064 x 2208 pixels per eye, a refresh rate of up to 120 Hz, and a field of view of 110 degrees. These devices provided stereoscopic visualization and interaction, dynamically adjusting the medical image data according to the user's movements and positional changes. During software operation, the HMD was connected to the computer via the built-in link functionality of the Meta Quest models.
\subsection{User Interaction and Interfaces}
As handheld controllers provide a more intuitive approach for interaction within a 3D virtual reality setting compared to conventional 2D controls, we implemented several interaction functions using the Meta Quest Touch Pro Controllers and Meta Quest 3 Touch Plus Controllers. Additionally, we developed an intuitive Graphical User Interface (GUI) to serve as a menu for segmented regions of the 3D models, anonymized patient profiles, and quick access to certain functions (Figure \ref{fig:GUI}). The core interactions implemented in our system included:
\begin{itemize}
  \item Continuous translation in all six degrees of freedom (6DoF)
  \item Continuous rotation in all three degrees of rotational freedom (3DoF)
  \item Selective visibility and transparency of individual segmented regions of the model
  \item Measurements of omnidirectional linear distance on the volume by placing start and end points
  \item Marking and drawing on the volume freely
\end{itemize}
\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{../Media/GUI}  
  \caption{Graphical User Interface.}
  \label{fig:GUI}
\end{figure}

A concurrent 2D slice image viewer was developed for comparison and correlation between 3D models and conventional medical images, including CT and MRI. A virtual cutting plane on the 3D models represented the corresponding level of the slice (Figure \ref{fig:CuttingPlane}) and translated accordingly when the user scrolled through the images.
\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{../Media/CuttingPlane.png}  
  \caption{Virtual cutting plane of 3D model and corresponding CT slice.}
  \label{fig:CuttingPlane}
\end{figure}
\subsection{Synchronous Sharing}
To enable real-time collaboration, education, and general communication, we develop a streaming feature that allows users without HMDs to join the preoperative planning system using conventional input devices and built-in browsers. Leveraging web real-time communication (WebRTC), we establish an extended reality (XR) cloud streaming service and a server. When the main user interacts with the system, updates are sent to the server, which then multicasts these updates to other clients \cite{RN12}. Additionally, audio from the main user can be broadcasted to remote audiences.
\section{Methods}
We design a pilot study with 10 participants, including 3 attending surgeons and 7 residents. The purpose of the study is to perform an initial validation of the system and the recorded data rather than to characterize the performance of the participants. Written informed consent is obtained from all patients included for VR collaborative surgical planning with the system described above. After an introduction and familiarization session with the system, all participating physicians evaluate the patient-specific models as the main user, with no time limitations. Each participant also joins an additional session using smartphones to participate online. This study is approved by Research Ethics Committee of our institutional review board under 202305019RINC (approval date 2023/10/23). Informed consent is obtained from all participants, including physicians and patients.

\section{Results}
The presented pipeline requires approximately an hour from importing raw data to presentation of 3D models in the VR system. This time estimate comes from running the program on the aforementioned computing resources and device. 
The segmentation and visual fidelity of the end product was verified in both conventional 2D displays and HMDs, which are rated functionally accurate by all partcipants. The results for one patient are shown in Figure \ref{fig:GUI}, \ref{fig:CuttingPlane} and video in Supplementary data. 
\section{Discussion}
Surgical interventions for complex pathologies are highly intricate, requiring precise planning to optimize outcomes while preserving critical structures. These challenges necessitate advanced preoperative planning solutions that enable detailed visualization and interaction with patient-specific anatomy. Our virtual reality collaborative preoperative planning system addresses these needs by providing a workflow for reconstructing patient-specific 3D anatomic models from 2D image data. This VR system facilitates collaborative interaction and annotation of these virtual models in an immersive environment.

The quality of the input data directly affects the fidelity of the 3D reconstructions in the virtual environment \cite{RN7, RN45}. To avoid loss of detail in the 3D models, acquisition and processing parameters of CT/MRI data need to be calibrated and standardized. Slice thickness of CT has been shown to be one of the primary factors affecting 3D model resolution and quality, with multiple studies proposing a maximum threshold of 1.25 mm \cite{RN7, RN47}. Consequently, we adopted a similar setting for input quality selection in our study.

The segmentation process is one of the most time- and resource-critical steps in generating anatomically accurate models. Manual segmentation by trained physicians or technicians has been extensively adopted in previous studies, achieving acceptable accuracy \cite{RN17, RN48, RN49}. However, the workload of manual tracing limits its efficiency, cost-effectiveness, and reproducibility. Semi-automatic or automatic segmentation aims to replace manual labor with computational processes. Methodologies including thresholding, neural networks, and other machine learning algorithms have been reported for segmenting normal tissues \cite{RN50} or pathologies \cite{RN52, RN54, RN55} across different regions. Semi-automatic segmentation alleviates the manual workload by efficiently producing segments of different organs while preserving the flexibility of manual editing for rare anatomical variations or complex pathologies. As demonstrated in our study, integrating semi-automatic segmentation into the generation of 3D-VR models has proven feasible and clinically useful for preoperative planning \cite{RN10, RN23}. A fully automatic process to segment multiple structures within a region of interest could potentially further improve efficiency and facilitate streamlining the process into an executable pipeline \cite{RN51, RN57, RN56}. However, there is still limited evidence on the feasibility of incorporating such a strategy into clinical planning \cite{RN9}.

Mesh optimization is essential in virtual reality (VR) to ensure smooth and immersive user experiences \cite{RN58}. Optimization techniques like polygon reduction and smoothing are crucial for managing the high computational demands of VR applications \cite{RN58, RN59}. Several open-source computer-aided design software packages are available and validated for generating and editing 3D medical models \cite{RN60, RN5}. In our study, we adopted MeshLab, which provides a wide range of advanced tools and functionalities and is supported by some of the largest online communities at present times \cite{RN61}.

Implementing multi-user collaboration in a virtual environment provides an innovative way for communication in medical settings. The benefits extend beyond preoperative planning into surgical simulation, personnel training, and education purposes \cite{RN64, RN26, RN65}. To maximize the advantages of collaborative features while addressing the cost of current HMDs, our system allows users to join via conventional devices with network connection and browsers. This approach not only ensures convenience and accessibility but also offers a comparable user experience for those using non-HMD devices, facilitating spectating and interaction without compromising the overall quality \cite{RN63}.


\section{Conclusion and Future Work}
We present the system architecture and technical setup of ColabSurgVR, a virtual reality collaborative system for preoperative planning. Our workflow involves creating detailed, patient-specific anatomical models from 2D image data and presenting them in an immersive, three-dimensional visualization platform. This interactive environment enhances the accuracy of surgical planning and enables real-time collaboration among multidisciplinary teams. Initial validation through a prospective pilot study with clinical participants demonstrated the system's accuracy and potential clinical benefits.
Future research will focus on enhancing the capabilities and usability of our VR system. One area of focus is optimizing computational processes to achieve rapid turnaround times suitable for emergent surgeries. Additionally, expanding and validating the VR system's application in clinical practice through comparative trials with more participants and patients will help assess its clinical impact on both surgeon-specific and patient outcomes. The proposed system could also be beneficial for a wide range of clinical applications beyond surgical planning, including physician training, student teaching, case collection, and patient communication.

\backmatter

\bmhead{Supplementary information}

If your article has accompanying supplementary file/s please state so here. 

Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

Please refer to Journal-level guidance for any specific requirements.

\bmhead{Acknowledgements}

We thank the patients who participated in this study, as well as all the research staff for their dedication to the research process.

\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding
\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
\item Ethics approval and consent to participate
\item Consent for publication
\item Data availability 
\item Materials availability
\item Code availability 
\item Author contribution
\end{itemize}

\noindent
If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
\bigskip
\begin{flushleft}%
Editorial Policies for:

\bigskip\noindent
Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

\bigskip\noindent
Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

\bigskip\noindent
\textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

\bigskip\noindent
BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
\end{flushleft}

\begin{appendices}

\section{Section title of first appendix}\label{secA1}

An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
